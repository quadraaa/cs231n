{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_std(x,axis=0):\n",
    "    print('  means: ', x.mean(axis=axis))\n",
    "    print('  stds:  ', x.std(axis=axis))\n",
    "    print() \n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta):\n",
    "    # x - N, M\n",
    "    x_avg = np.mean(x, axis=0)\n",
    "    x_sqwrd = x**2\n",
    "    x_avg_sqwrd = x_avg**2\n",
    "    x_sqwrd_avg = np.mean(x_sqwrd, axis=0)\n",
    "    v = x_sqwrd_avg - x_avg_sqwrd\n",
    "    v_shifted = v + 1e-5\n",
    "    denom = np.sqrt(v_shifted)\n",
    "    x_cntrd = x - x_avg\n",
    "    x_stndrd = x_cntrd/denom\n",
    "    x_strchd = x_stndrd * gamma\n",
    "    x_rstrd = x_strchd + beta\n",
    "    cache = {\"x\": x,\n",
    "            \"x_avg\":x_avg,\n",
    "            \"x_sqwrd\":x_sqwrd,\n",
    "            \"x_avg_sqwrd\":x_avg_sqwrd,\n",
    "            \"x_sqwrd_avg\":x_sqwrd_avg,\n",
    "            \"v\":v,\n",
    "            \"v_shifted\":v_shifted,\n",
    "            \"denom\":denom,\n",
    "            \"x_cntrd\":x_cntrd,\n",
    "            \"x_stndrd\":x_stndrd,\n",
    "            \"x_strchd\":x_strchd,\n",
    "            \"x_rstrd\":x_rstrd,\n",
    "             \"gamma\":gamma,\n",
    "             \"beta\":beta\n",
    "            }\n",
    "    return x_rstrd, cache\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    # x - N, M\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dx_strchd = dout\n",
    "    dgamma = np.sum(dx_strchd * cache[\"x_stndrd\"], axis=0)\n",
    "    dx_stndrd = dx_strchd * cache[\"gamma\"]\n",
    "    dx_cntrd = dx_stndrd * (1/cache[\"denom\"])\n",
    "    ddenom = np.sum(dx_stndrd * (-1*cache[\"x_cntrd\"]*cache[\"denom\"]**(-2)), axis=0)\n",
    "    dv_shifted = ddenom * (0.5*(cache[\"v_shifted\"]**(-0.5)))\n",
    "    dv = dv_shifted\n",
    "    dx_avg_sqwrd = -1 * dv\n",
    "    dx_avg = dx_avg_sqwrd * 2 * cache[\"x_avg\"] + np.sum(dx_cntrd * (-1), axis = 0)\n",
    "    dx_sqwrd_avg = dv\n",
    "    dx_sqwrd = dx_sqwrd_avg * np.ones(cache[\"x_sqwrd\"].shape)/cache[\"x_sqwrd\"].shape[0]\n",
    "    dx = dx_sqwrd * 2 * cache[\"x\"] + \\\n",
    "         dx_avg * np.ones(cache[\"x\"].shape)/cache[\"x\"].shape[0] + \\\n",
    "         dx_cntrd\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward_1(x, gamma, beta, replacements={}):\n",
    "    # x - N, M\n",
    "    x_avg = replacements.get(\"x_avg\", np.mean(replacements.get(\"x\", x), axis=0))\n",
    "    x_sqwrd = replacements.get(\"x_sqwrd\", replacements.get(\"x\", x)**2)\n",
    "    x_avg_sqwrd = replacements.get(\"x_avg_sqwrd\", x_avg**2)\n",
    "    x_sqwrd_avg = replacements.get(\"x_sqwrd_avg\", np.mean(x_sqwrd, axis=0))\n",
    "    v = replacements.get(\"v\", x_sqwrd_avg - x_avg_sqwrd)\n",
    "    v_shifted = replacements.get(\"v_shifted\", v + 1e-5)\n",
    "    denom = replacements.get(\"denom\", np.sqrt(v_shifted))\n",
    "    x_cntrd = replacements.get(\"x_cntrd\", replacements.get(\"x\", x) - x_avg)\n",
    "    x_stndrd = replacements.get(\"x_stndrd\", x_cntrd/denom)\n",
    "    x_strchd = replacements.get(\"x_strchd\", x_stndrd * replacements.get(\"gamma\", gamma))\n",
    "    x_rstrd = replacements.get(\"x_rstrd\", x_strchd + replacements.get(\"beta\", beta))\n",
    "    cache = {\"x\": replacements.get(\"x\", x),\n",
    "            \"x_avg\":replacements.get(\"x_avg\", x_avg),\n",
    "            \"x_sqwrd\":replacements.get(\"x_sqwrd\", x_sqwrd),\n",
    "            \"x_avg_sqwrd\":replacements.get(\"x_avg_sqwrd\", x_avg_sqwrd),\n",
    "            \"x_sqwrd_avg\":replacements.get(\"x_sqwrd_avg\", x_sqwrd_avg),\n",
    "            \"v\":replacements.get(\"v\", v),\n",
    "            \"v_shifted\":replacements.get(\"v_shifted\", v_shifted),\n",
    "            \"denom\":replacements.get(\"denom\", denom),\n",
    "            \"x_cntrd\":replacements.get(\"x_cntrd\", x_cntrd),\n",
    "            \"x_stndrd\":replacements.get(\"x_stndrd\", x_stndrd),\n",
    "            \"x_strchd\":replacements.get(\"x_strchd\", x_strchd),\n",
    "            \"x_rstrd\":replacements.get(\"x_rstrd\", x_rstrd),\n",
    "             \"gamma\":replacements.get(\"gamma\", gamma),\n",
    "             \"beta\":replacements.get(\"beta\", beta)\n",
    "            }\n",
    "    return x_rstrd, cache\n",
    "\n",
    "def batchnorm_backward_1(dout, cache):\n",
    "    # x - N, M\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dx_strchd = dout\n",
    "    dgamma = np.sum(dx_strchd * cache[\"x_stndrd\"], axis=0)\n",
    "    dx_stndrd = dx_strchd * cache[\"gamma\"]\n",
    "    dx_cntrd = dx_stndrd * (1/cache[\"denom\"])\n",
    "    ddenom = np.sum(dx_stndrd * (-1*cache[\"x_cntrd\"]*cache[\"denom\"]**(-2)), axis=0)\n",
    "    dv_shifted = ddenom * (0.5*(cache[\"v_shifted\"]**(-0.5)))\n",
    "    dv = dv_shifted\n",
    "    dx_avg_sqwrd = -1 * dv\n",
    "    dx_avg = dx_avg_sqwrd * 2 * cache[\"x_avg\"] + np.sum(dx_cntrd * (-1), axis = 0)\n",
    "    dx_sqwrd_avg = dv\n",
    "    dx_sqwrd = dx_sqwrd_avg * np.ones(cache[\"x_sqwrd\"].shape)/cache[\"x_sqwrd\"].shape[0]\n",
    "    dx = dx_sqwrd * 2 * cache[\"x\"] + \\\n",
    "         dx_avg * np.ones(cache[\"x\"].shape)/cache[\"x\"].shape[0] + \\\n",
    "         dx_cntrd\n",
    "    grad = {\"dx\": dx,\n",
    "            \"dx_avg\":dx_avg,\n",
    "            \"dx_sqwrd\":dx_sqwrd,\n",
    "            \"dx_avg_sqwrd\":dx_avg_sqwrd,\n",
    "            \"dx_sqwrd_avg\":dx_sqwrd_avg,\n",
    "            \"dv\":dv,\n",
    "            \"dv_shifted\":dv_shifted,\n",
    "            \"ddenom\":ddenom,\n",
    "            \"dx_cntrd\":dx_cntrd,\n",
    "            \"dx_stndrd\":dx_stndrd,\n",
    "            \"dx_strchd\":dx_strchd,\n",
    "             \"dgamma\":dgamma,\n",
    "             \"dbeta\":dbeta\n",
    "            }\n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4.8611279461042055e-05\n"
     ]
    }
   ],
   "source": [
    "# check any\n",
    "N = 10\n",
    "D = 5\n",
    "delta = 1e-7\n",
    "replacements = {}\n",
    "\n",
    "inspected = \"x\"\n",
    "replacements[inspected] = (np.random.rand(N,D) + 1)/100\n",
    "\n",
    "x_dummy = np.random.rand(N,D)\n",
    "gamma = np.ones(D) + 0.1\n",
    "beta = np.zeros(D) + 0.1\n",
    "\n",
    "out, cache = batchnorm_forward_1(x_dummy, gamma, beta, replacements)\n",
    "w = np.random.rand(N,D)\n",
    "dout = w\n",
    "s = np.sum(out*w)\n",
    "grad = batchnorm_backward_1(dout, cache=cache)\n",
    "\n",
    "\n",
    "\n",
    "if len(replacements[inspected].shape) == 2:\n",
    "    print(2)\n",
    "    grad_manual = np.zeros((N, D))\n",
    "    for i in range(N):\n",
    "        for j in range(D):\n",
    "            v_shifted_mod = copy.deepcopy(replacements)\n",
    "            v_shifted_mod[inspected][i, j] = v_shifted_mod[inspected][i, j] + delta\n",
    "            out_mod, cache_mod = batchnorm_forward_1(x_dummy, gamma, beta, v_shifted_mod)\n",
    "            s_mod = np.sum(out_mod*w)\n",
    "            grad_manual[i, j] = (s_mod - s)/delta\n",
    "    print(rel_error(grad_manual,grad[\"d\"+inspected]))\n",
    "    \n",
    "elif len(replacements[inspected].shape) == 1:\n",
    "    print(1)\n",
    "    grad_manual = np.zeros((D))\n",
    "    for j in range(D):\n",
    "        v_shifted_mod = copy.deepcopy(replacements)\n",
    "        v_shifted_mod[inspected][j] = v_shifted_mod[inspected][j] + delta\n",
    "        #print(v_shifted_mod[inspected]-replacements[inspected])\n",
    "        out_mod, cache_mod = batchnorm_forward_1(x_dummy, gamma, beta, v_shifted_mod)\n",
    "        s_mod = np.sum(out_mod*w)\n",
    "        grad_manual[j] = (s_mod - s)/delta\n",
    "    print(rel_error(grad_manual,grad[\"d\"+inspected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:   [27.18502186 34.21455511 37.68611762]\n",
      "\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  means:  [4.66293670e-17 3.55271368e-17 1.85962357e-17]\n",
      "  stds:   [0.99999999 1.         1.        ]\n",
      "\n",
      "After batch normalization (gamma= [1. 2. 3.] , beta= [11. 12. 13.] )\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:   [0.99999999 1.99999999 2.99999999]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta)\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta)\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  6.6664407728161185e-09\n",
      "dgamma error:  2.2067811910924764e-12\n",
      "dbeta error:  2.6853898000928757e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward_1(x, gamma, beta)[0]\n",
    "fg = lambda a: batchnorm_forward_1(x, a, beta)[0]\n",
    "fb = lambda b: batchnorm_forward_1(x, gamma, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward_1(x, gamma, beta)\n",
    "grad_for_check = batchnorm_backward_1(dout, cache)\n",
    "dx, dgamma, dbeta = grad_for_check[\"dx\"], grad_for_check[\"dgamma\"], grad_for_check[\"dbeta\"]\n",
    "#You should expect to see relative errors between 1e-13 and 1e-8\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
